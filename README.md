# What is this repo?

This repo shows how to setup deploying deepseek with vllm to modal. 

Vllm is an open ai compatible backendserver for llms. Using vLLM we can query the vLLM server in the same way we would query openai, but its served by Deepseek model weights in the backend. And running it on modal, we can easily scale the backend anyway we like. Add more GPUs, sure, add more servers. No problem. Modal is great! :) 


TODO: Add blogpost about this.
